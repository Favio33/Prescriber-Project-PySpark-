# PySpark Project - GCP


## Introduction

This spark project was developed to be hosted in a Compute Engine in GCP where a Hadoop Cluster is set up. We use docker to create a PostgreSQL database to set up Hive.

## Requirements

* The required files to test in local are not in the repository. Please contact me to send you the link (https://www.linkedin.com/in/favio-varillas/)
* Must set up Spark and Hadoop locally to test source code

## Considerations

* Modify path files in /spark/bash according to local/user and GCP Project
